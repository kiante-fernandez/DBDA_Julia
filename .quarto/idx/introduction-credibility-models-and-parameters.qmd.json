{"title":"Introduction: Credibility, Models, and Parameters","markdown":{"yaml":{"format":{"html":{"code-fold":false}},"jupyter":"julia-1.7"},"headingText":"Introduction: Credibility, Models, and Parameters","containsRefs":false,"markdown":"\n\n\n## Bayesian inference is reallocation of credibility across possibilities\n\nTo make Figure 2.1 we need data. We will create some synthetic data and store it in using `DataFrames.jl`\n\n```{julia}\n#| warning: false\n#| output: false\n\nusing DataFrames\n\nfunction expand_grid(; iters...)\n    var_names = collect(keys(iters))\n    var_itr = [1:length(x) for x in iters.data]\n    var_ix = vcat([collect(x)' for x in Iterators.product(var_itr...)]...)\n    out = DataFrame()\n    for i = 1:length(var_names)\n        out[:,var_names[i]] = collect(iters[i])[var_ix[:,i]]\n    end\n    return out\nend\n\nd = expand_grid(iteration=1:3, Possibilities=[\"A\", \"B\",\"C\", \"D\"], stage = [\"Prior\", \"Posterior\"])\n\nd2 =DataFrame(Credibility =[fill(.25,4); 0; fill(1/3,3); 0; fill(1/3,3);0;.5;0;0.5;0;.5;0;0.5;fill(0,3);1])\n\nsort!(d, [:iteration])\nd.Credibility = d2.Credibility\n\n```\n\nHere we are defining a function that creates combinations and then I just bind that we the Credibility values we will use for plotting. \n\nWe can take a look at the top few rows of the data with the `first()` function.\n\n```{julia}\nfirst(d,5)\n```\n\nNow lets plot our version of Figure 2.1. To do so, we will use the `Gadfly.jl` package. This is my preferred plotting package because it has similar syntax to ggplot2 package in R.\n\n\n```{julia}\n#| warning: false\n#| echo: false\n\nusing Gadfly\nusing DataFramesMeta\n\n#Specify the plot size\nset_default_plot_size(16cm, 18cm)\n\ndf_Prior = @where(d, :stage .== \"Prior\")\ndf_Posterior = @where(d, :stage .== \"Posterior\")\n\nfig1a = plot(df_Prior,\n    xgroup = :iteration,\n    x = :Possibilities,\n    y = :Credibility,\n    Geom.subplot_grid(Geom.bar(orientation = :vertical),\n    Guide.xlabel(orientation= :horizontal),\n    Guide.xticks(orientation = :horizontal)),\n    Guide.xlabel(\"Possibilities\"),\n    Guide.ylabel(\"Credibility\"),\n    Guide.title(\"Prior\"),\n    Scale.y_continuous(format = :plain),\n    Theme(\n        background_color = \"white\",\n        bar_spacing = 1mm,\n        grid_color = \"white\"\n    )\n)\nfig1b = plot(df_Posterior,\n    xgroup = :iteration,\n    x = :Possibilities,\n    y = :Credibility,\n    Geom.subplot_grid(Geom.bar(orientation = :vertical),\n    Guide.xlabel(orientation= :horizontal),\n    Guide.xticks(orientation = :horizontal)),\n    Guide.xlabel(\"Possibilities\"),\n    Guide.ylabel(\"Credibility\"),\n    Guide.title(\"Posterior\"),\n    Scale.y_continuous(format = :plain),\n    Theme(\n        background_color = \"white\",\n        bar_spacing = 1mm,\n        grid_color = \"white\"\n    )\n)\n#create a subplot\nvstack(fig1a,fig1b)\n```\n\nmaybe add the annotation using something like below\nhttps://stackoverflow.com/questions/50925450/gadfly-julia-how-to-add-text-annotation-to-a-plot\n\n### Data are noisy and inferences are probabilistic.\n\nThe next plot will involve getting creating some Gaussian distributions. To do so, we will use the `Distributions.jl` package. Distribution allows create and extract information from a wide range of probability distributions. We will use this package a lot in subsequent chapter to draw samples to facilitate Bayesian inference. Note that I am ignoring the scaling issue apparent from the four distributions densities not being equal to one. Also to create the illusion of a discrete x axis, I will just show only the 4 values. \n\nTODO: rescale the curves such that the density on the y axis for each curve sums to one.\n\n```{julia}\nusing Gadfly\nusing Distributions\nusing DataFrames\n\nset_default_plot_size(16cm, 18cm)\n\nticks = [1, 2, 3, 4] #specify the tick marks\n\n#create some data for the bars\nx5 = DataFrame(Possibilities = 1, Credibility = pdf(Normal(1, 1.2), 1))\nx6 = DataFrame(Possibilities = 2, Credibility = pdf(Normal(2, 1.2), 2))\nx7 = DataFrame(Possibilities = 3, Credibility = pdf(Normal(3, 1.2), 3))\nx8 = DataFrame(Possibilities = 4, Credibility = pdf(Normal(4, 1.2), 4))\n\nplot(layer(x5, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x6, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x7, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x8, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x->pdf(Normal(1, 1.2), x), -6, 10, color=[colorant\"black\"], order=1),\n    layer(x->pdf(Normal(2, 1.2), x), -6, 10, color=[colorant\"black\"],order=1),\n    layer(x->pdf(Normal(3, 1.2), x), -6, 10, color=[colorant\"black\"],order=1),\n    layer(x->pdf(Normal(4, 1.2), x), -6, 10, color=[colorant\"black\"],order=1),\n    Guide.xticks(ticks=ticks),\n    Guide.xlabel(\"Possibilities\"),\n    Guide.ylabel(\"Credibility\"),\n    Guide.title(\"Prior\"),\n     Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        bar_spacing = 1mm,\n        key_position = :none\n    ))\n\n```\n\nNow we can use the same method to create the bottom panel of Figure 2.3. Here I will just hide the y axis. What is important to note is just how the assignment of credibility changes based on our 3 observations (shown in red). \n\n```{julia}\n\nticks = [1, 2, 3, 4] #specify the tick marks\n\n#create some data for the bars\nx5 = DataFrame(Possibilities = 1, Credibility = pdf(Normal(1, 8), 1))\nx6 = DataFrame(Possibilities = 2, Credibility = pdf(Normal(2, 3), 2))\nx7 = DataFrame(Possibilities = 3, Credibility = pdf(Normal(3, 5), 3))\nx8 = DataFrame(Possibilities = 4, Credibility = pdf(Normal(4, 20), 4))\nx9 = DataFrame(Possibilities = [1.75, 2.25, 2.75], Credibility = zeros(3))\n\nplot(layer(x5, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x6, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x7, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x8, x = :Possibilities, y = :Credibility, Geom.bar(orientation = :vertical)),\n    layer(x->pdf(Normal(1, 8), x), -6, 10, color=[colorant\"black\"], order=1),\n    layer(x->pdf(Normal(2, 3), x), -6, 10, color=[colorant\"black\"],order=1),\n    layer(x->pdf(Normal(3, 5), x), -6, 10, color=[colorant\"black\"],order=1),\n    layer(x->pdf(Normal(4, 20), x), -6, 10, color=[colorant\"black\"],order=1),\n    layer(x9, x = :Possibilities, y = :Credibility, Geom.point, color=[colorant\"red\"],size =[.2], order=2), #add data\n    Guide.xticks(ticks=ticks),\n    Guide.yticks(ticks=[]),\n    Guide.xlabel(\"Possibilities\"),\n    Guide.ylabel(\"Credibility\"),\n    Guide.title(\"Posterior\"),\n    Coord.Cartesian(ymin=0,ymax=0.3),\n     Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        bar_spacing = 1mm,\n        key_position = :none\n    ))\n```\n\n\n## Possibilities are parameter values in descriptive models\n\nIn the last section, we used the `Normal()` function to make curves following Gaussian distribution. Now we will showcase the power and flexibility of the `Distribution.jl` package by assigning a distribution to a name and drawing random samples from that object. By doing so we can simulate data from a the specified normal distribution.\n\n```{julia}\nusing Random\n# set the seed to make the simulation reproducible\nRandom.seed!(2022) \n\n# assign d `Normal()` with μ 10 σ = 5 \nd = Normal(10,5)\n# simulate the data with `rand()` from the specified distribution\nsimulated_data = DataFrame(x = rand(d, 2000))\n\n#plot it\n#Figure 2.4.a.\nplot(layer(simulated_data, x=:x, Geom.histogram(density=true)),\n    layer(x->pdf(Normal(10, 5), x), -8, 27, color=[colorant\"black\"],order=1),\n    Guide.xlabel(\"Data Values\"),\n    Guide.ylabel(\"Data Probability\"),\n    Guide.title(\"The candidate normal distribution\\nhas a μ of 10 and σ of 5.\"),\n    Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        line_width = 1mm\n    )\n)\n```\n\n```{julia}\n#Figure 2.4.b.\nplot(layer(simulated_data, x=:x, Geom.histogram(density=true)),\n    layer(x->pdf(Normal(8, 6), x), -8, 27, color=[colorant\"black\"],order=1),\n    Guide.xlabel(\"Data Values\"),\n    Guide.ylabel(\"Data Probability\"),\n    Guide.title(\"The candidate normal distribution\\nhas a μ of 8 and σ of 6.\"),\n    Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        line_style = [:dash],\n        line_width = 1mm\n    )\n)\n```\n\n\n## The steps of Bayesian data analysis\n\n> In general, Bayesian analysis of data follows these steps:\n>\n> 1. Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?\n> 2. Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.\n> 3. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.\n> 4. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).\n> 5. Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model.\n>\n> Perhaps the best way to explain these steps is with a realistic example of Bayesian data analysis. The discussion that follows is abbreviated for purposes of this introductory chapter, with many technical details suppressed. (p. 25)\n\nI will showcase the entire workflow here. In later chapters we’ll cover this workflow in much more detail. \n\nFirst we need to generate the data and fit a model to those data. In `HtWtDataDenerator.R` script, Kruschke provided the code for a function that will simulate height/weight data. Lets rewrite that function in Julia:\n\n```{julia}\nusing Distributions, Random, DataFrames, StatsBase\n\nfunction HtWtDataGenerator(nSubj, rndsd = nothing, maleProb = 0.50) \n    # Random height, weight generator for males and females. Uses parameters from\n    # Brainard, J. & Burmaster, D. E. (1992). Bivariate distributions for height and\n    # weight of men and women in the United States. Risk Analysis, 12(2), 267-275.\n    # Kruschke, J. K. (2011). Doing Bayesian data analysis:\n    # A Tutorial with R and BUGS. Academic Press / Elsevier.\n    # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition:\n    # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier.\n    \n    # Specify parameters of multivariate normal (MVN) distributions.\n  \n    # Men:\n    HtMmu   = 69.18\n    HtMsd   = 2.87\n    lnWtMmu = 5.14\n    lnWtMsd = 0.17\n    Mrho    = 0.42\n    Mmean   = [HtMmu, lnWtMmu]\n    Msigma  = [HtMsd^2 Mrho * HtMsd * lnWtMsd; Mrho * HtMsd * lnWtMsd lnWtMsd^2]\n    # Women cluster 1:\n    HtFmu1   = 63.11\n    HtFsd1   = 2.76\n    lnWtFmu1 = 5.06\n    lnWtFsd1 = 0.24\n    Frho1    = 0.41\n    prop1    = 0.46\n    Fmean1   = [HtFmu1, lnWtFmu1]\n    Fsigma1  = [HtFsd1^2 Frho1 * HtFsd1 * lnWtFsd1; Frho1 * HtFsd1 * lnWtFsd1  lnWtFsd1^2]\n    # Women cluster 2:\n    HtFmu2   = 64.36\n    HtFsd2   = 2.49\n    lnWtFmu2 = 4.86\n    lnWtFsd2 = 0.14\n    Frho2    = 0.44\n    prop2    = 1 - prop1\n    Fmean2   = [HtFmu2, lnWtFmu2]\n    Fsigma2  = [HtFsd2^2  Frho2 * HtFsd2 * lnWtFsd2; Frho2 * HtFsd2 * lnWtFsd2  lnWtFsd2^2]\n\n    # Randomly generate data values from those MVN distributions.\n    if rndsd !== nothing \n        Random.seed!(rndsd) \n    end\n    datamatrix =  DataFrame(zeros(nSubj, 3), [\"male\", \"height\", \"weight\"])\n    # arbitrary coding values\n    maleval = 1\n    femaleval = 0\n\n    for i in 1:nSubj\n        # Flip coin to decide sex\n        sex = wsample([maleval, femaleval], [maleProb, 1 - maleProb], 1)\n\n        if sex[1] == maleval\n            datum = rand(MvNormal(Mmean, Msigma))\n        elseif sex[1] == femaleval\n            Fclust = wsample([1, 2], [prop1, prop2], 1)\n\n            if Fclust[1] == 1\n            datum = rand(MvNormal(Fmean1, Fsigma1))\n            else\n            datum = rand(MvNormal(Fmean2, Fsigma2))\n            end\n        end\n        datum[2] = exp(datum[2])\n        datamatrix[i, :] = [sex; round.(datum, digits = 1)]\n    end\n\n    return datamatrix\n\nend\n```\n\nThe `HtWtDataGenerator()` function has two arguments. The nSubj argument determines how many values to generate, and  maleProb determines how probable we want those values to be from men.\n\n```{julia}\n#| warning: false\n#| echo: false\n\n# set your seed to make the data generation reproducible\nRandom.seed!(2022) \n\nd = HtWtDataGenerator(57, 2022, 0.5)\n\nfirst(d,5)\n```\n\nWe’re about ready for the model, which we will fit it with the Hamiltonian Monte Carlo (HMC) method via the turing.jl package. We’ll introduce turing.jl more fully in Chapter 8. I also recommend you go check out the following and resources:\n\n- [https://turing.ml/v0.21/tutorials/00-introduction/](https://turing.ml/v0.21/tutorials/00-introduction/)\n- [http://hakank.org/julia/turing/](http://hakank.org/julia/turing/)\n- [https://storopoli.github.io/Bayesian-Julia/](https://storopoli.github.io/Bayesian-Julia/)\n\n\nFollowing [Solomon Kurz's example](https://bookdown.org/content/3686/introduction-credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis), we’ll use weakly-regularizing priors for the intercept and slope and a half Cauchy with a fairly large scale parameter for $\\sigma$\n\n```{julia}\n#| warning: false\n\nusing Turing\nusing Optim\nusing MCMCChains, Plots, StatsPlots, Gadfly\nusing AbstractMCMC\n# Define the model\n# linear regression.\n@model function linear_regression(x,y)\n    n = length(y)\n\n    # Set variance prior.\n    σ ~  Truncated(Cauchy(0,10),0,Inf)\n    # Set intercept prior.\n    α ~ Normal(0,100)\n    # Set the prior for beta.\n    β  ~ Normal(0,100)\n\n    for i in 1:n\n        y[i] ~ Normal(α + β * x[i], σ)\n    end\n\n    return y\nend\n\n#to be concrete we assign the values to x and y\nx = d.height\ny = d.weight\n\nmodel = linear_regression(x, y)\n\n#  Run sampler, collect results\nchain_lin_reg = sample(model, NUTS(500, 0.65),MCMCDistributed(),2_000, 4);\n```\n\nIf you wanted a model summary, you could execute `display(chain_lin_reg)`. For more detail, see Chapter 8. Here’s Figure 2.5.a.\n\n```{julia}\n#store in a Dataframe\nplot_df = DataFrame(sample(chain_lin_reg[:,:,:], 57))\nplot_df.x = x\nplot_df.y = y\n\n#plot\nabline = Geom.abline(color=\"red\", style=:dash)\nGadfly.plot(plot_df,\n    x=:x, y=:y, Geom.point,color = [colorant\"black\"], intercept=:α, slope=:β, abline, \n    Guide.xlabel(\"Height in inches\"),\n    Guide.ylabel(\"Weight in pounds\"),\n    Guide.title(\"Data with 57 credible regression lines\"),\n    Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        line_style = [:dash],\n        line_width = .1mm\n    ))\n```\n\nFor Figure 2.5.b., we’ll mark off the mode and 95% highest density interval (HDI).\n\n```{julia}\ndf = DataFrame(chain_lin_reg)\n\nplt_HDI = DataFrame(quantile(group(chain_lin_reg, :β)))\nplt_HDI.β .= mode(chain_lin_reg[:,:β,:])\n\nplot(layer(df, x=:β, Geom.histogram(bincount = 50)),\n    layer(plt_HDI, x = :β,xmin=\"2.5%\", xmax=\"97.5%\",Geom.point, Geom.errorbar, color=[colorant\"red\"],size =[.1], order=1),\n    Guide.xlabel(\"β(slope)\"),\n    Guide.title(\"The posterior distribution \\n\n    The mode and 95% HDI intervals are\\nthe dot and horizontal line at the bottom.\"),\n    Guide.yticks(ticks=[]),\n    Theme(\n        background_color = \"white\",\n        grid_color = \"white\", \n        bar_spacing = 0.25mm,\n        line_width = 1mm\n))\n\n```\n\nTo make Figure 2.6, we use the `predict()` function. First we create a model object with missing values for our dependent variable, then we will predict those values using our posterior samples.\n\n```{julia}\n# extract the posterior draws\nmodel_pred = linear_regression(x,Vector{Missing}(missing, length(y)))\nposterior_check = predict(model_pred, chain_lin_reg)\n\n#get the summary stats for plotting\nposterior_res = summarystats(posterior_check)\n#create DataFrame\nplot_df = DataFrame(quantile(posterior_check))\nplot_df.x = x\nplot_df.y = y\nplot_df.predictions = posterior_res[:,:mean]\n\n#plot\nGadfly.plot(\n    layer(plot_df, x=:x, y=:y, Geom.point,color = [colorant\"black\"]),\n    layer(plot_df, x=:x, y=:predictions,ymin=\"2.5%\", ymax=\"97.5%\",Geom.point,Geom.errorbar, color = [colorant\"grey\"]),Guide.xlabel(\"Height in inches\"),\n    Guide.ylabel(\"Weight in pounds\"),\n    Guide.title(\"Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions\"),\n    Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        line_width = .1mm\n    ))\n```\n\nThe posterior predictions might be easier to depict with a ribbon and line, instead. Luckily `Gadfly.jl` makes it really easy to change. \n\n```{julia}\nGadfly.plot(\n    layer(plot_df, x=:x, y=:y, Geom.point,color = [colorant\"black\"]),\n    layer(plot_df, x=:x, y=:predictions,ymin=\"2.5%\", ymax=\"97.5%\",Geom.line,Geom.ribbon, color = [colorant\"red\"]),Guide.xlabel(\"Height in inches\"),\n    Guide.ylabel(\"Weight in pounds\"),\n    Guide.title(\"Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions\"),\n    Theme(\n        background_color = \"white\",\n        grid_color = \"white\",\n        line_width = .2mm\n    ))\n```\n\n> “We have seen the five steps of Bayesian analysis in a fairly realistic example. This book explains how to do this sort of analysis for many different applications and types of descriptive models” (p. 30). \nThat's it! We can now start digging into the details that make up each of the steps outlined in this introductory chapter. Learning Bayesian inference is a rewarding experience. I wish you the best of luck on this exciting journey! \n\n## Session info\n```{julia}\nPkg.status()\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"introduction-credibility-models-and-parameters.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.38","bibliography":["references.bib"],"theme":"cosmo","cover-image":"DBDA2Ecover.png","jupyter":"julia-1.7"},"extensions":{"book":{"multiFile":true}}}}}